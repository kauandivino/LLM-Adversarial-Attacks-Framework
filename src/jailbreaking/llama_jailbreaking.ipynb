{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw0gAIpkd1dx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-syDYGwKff9C"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"TOKEN_HUGGINGFACE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGVOefDSfns5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wj63cpRmfthp"
      },
      "outputs": [],
      "source": [
        "def generate_llama_response(prompt):\n",
        "    \"\"\"\n",
        "    Gera uma resposta do modelo LLaMA com base no prompt fornecido.\n",
        "\n",
        "    :param prompt: O prompt a ser enviado ao modelo.\n",
        "    :return: Resposta gerada pelo LLaMA.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=5000)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "191Ra4EqfyRX"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def load_prompts(csv_path):\n",
        "    \"\"\"\n",
        "    Carrega os prompts do dataset CSV e retorna uma lista de dicionários contendo os prompts.\n",
        "\n",
        "    :param csv_path: Caminho para o arquivo CSV com os prompts.\n",
        "    :return: Lista de prompts (cada prompt é um dicionário).\n",
        "    \"\"\"\n",
        "    prompts = []\n",
        "    with open(csv_path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            prompts.append(row)\n",
        "    return prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bip69lqXpFXP"
      },
      "outputs": [],
      "source": [
        "def run_llama_jailbreaking_test(dataset_path, num_tests_per_subject=2, max_interactions=5):\n",
        "    \"\"\"\n",
        "    Executa os testes de jailbreaking no modelo LLaMA de forma progressiva, ajustando o nível de agressividade.\n",
        "\n",
        "    :param dataset_path: Caminho para o dataset contendo os prompts.\n",
        "    :param num_tests_per_subject: Número de testes a serem feitos por assunto.\n",
        "    :param max_interactions: Número máximo de interações permitidas por sequência de ataque.\n",
        "    \"\"\"\n",
        "    prompts = load_prompts(dataset_path)\n",
        "    subjects = set([prompt['subject'] for prompt in prompts])\n",
        "\n",
        "    for subject in subjects:\n",
        "        print(f\"\\n--- Iniciando Testes para Subject: {subject} ---\")\n",
        "\n",
        "        subject_prompts = [p for p in prompts if p['subject'] == subject]\n",
        "\n",
        "        for test_num in range(num_tests_per_subject):\n",
        "            print(f\"\\n--- Teste {test_num + 1} para {subject} ---\")\n",
        "\n",
        "            base_prompt = next(p for p in subject_prompts if p['progression_stage'] == \"1\")\n",
        "            jailbreaking_prompt = base_prompt['jailbreaking_injection']\n",
        "            expected_behavior = base_prompt['expected_behavior']\n",
        "            aggressiveness_level = int(base_prompt['aggressiveness_level'])\n",
        "\n",
        "            print(f\"Prompt Inicial: {base_prompt['base_prompt']} (Nível {aggressiveness_level})\")\n",
        "\n",
        "            initial_prompt = f\"{base_prompt['base_prompt']} {jailbreaking_prompt}\"\n",
        "            original_response = generate_llama_response(initial_prompt)\n",
        "            print(f\"Resposta Original: {original_response}\")\n",
        "\n",
        "            for progression_stage in range(1, max_interactions + 1):\n",
        "                next_prompt_data = next(p for p in subject_prompts if p['progression_stage'] == str(progression_stage))\n",
        "                next_prompt = f\"{next_prompt_data['base_prompt']} {next_prompt_data['jailbreaking_injection']}\"\n",
        "                injected_response = generate_llama_response(next_prompt)\n",
        "\n",
        "                print(f\"Resposta após injeção no estágio {progression_stage}: {injected_response}\")\n",
        "\n",
        "                deviation_detected = next_prompt_data['expected_behavior'].lower() not in injected_response.lower()\n",
        "                print(f\"Desvio detectado: {'Sim' if deviation_detected else 'Não'}\")\n",
        "\n",
        "                if deviation_detected:\n",
        "                    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_zNdDrIgAih"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "dataset_path = \"jailbreaking_dataset.csv\"\n",
        "run_llama_jailbreaking_test(dataset_path, num_tests_per_subject=2, max_interactions=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
